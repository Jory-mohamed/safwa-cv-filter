import streamlit as st
import re
import unicodedata
from pypdf import PdfReader
import io

# ========= Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© =========
st.set_page_config(page_title="ÙØ±Ø² Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ© - HR Filter", page_icon="ğŸ—‚ï¸", layout="centered")
st.title("ğŸ—‚ï¸ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙØ±Ø² Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ© (Ù†Ø³Ø®Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ©)")
st.caption("ÙŠØªØ­Ù‚Ù‚ Ù…Ù†: **Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯** + **Ù†Ø¸Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©** + **Ø§Ù„Ø¬Ù†Ø³ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©**")

# ========= Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© =========
def normalize_ar(text: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ/Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©."""
    if not text:
        return ""
    text = text.lower()
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = ''.join(ch for ch in unicodedata.normalize('NFKD', text) if not unicodedata.combining(ch))
    # ØªÙˆØ­ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ
    text = re.sub(r"[Ø£Ø¥Ø¢Ù±]", "Ø§", text)
    text = text.replace("Ø©", "Ù‡").replace("Ù‰", "ÙŠ")
    # Ù…Ø³Ø§ÙØ§Øª ÙˆØ¹Ù„Ø§Ù…Ø§Øª Ø²Ø§Ø¦Ø¯Ø©
    text = re.sub(r"[^0-9a-z\u0600-\u06FF\s]+", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_pdf_text(file_bytes: bytes) -> str:
    """Ù‚Ø±Ø§Ø¡Ø© Ù†Øµ Ø§Ù„Ù€ PDF (Ø¨Ø¯ÙˆÙ† OCR)."""
    reader = PdfReader(io.BytesIO(file_bytes))
    pages = []
    for p in reader.pages:
        try:
            pages.append(p.extract_text() or "")
        except Exception:
            pages.append("")
    return "\n".join(pages)

# ========= Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø­Ø« (Ù‚ÙˆØ§Ø¹Ø¯) =========
# Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯
UNI_PATTERNS = [
    r"\bØ¬Ø§Ù…Ø¹Ù‡?\s+Ø§Ù„Ù…Ù„Ùƒ\s+Ø³Ø¹ÙˆØ¯\b",
    r"\bking\s+saud\s+university\b",
    r"\bksu\b"
]

# Ù†Ø¸Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ© (Ù…Ø±Ø§Ø¯ÙØ§Øª Ø´Ø§Ø¦Ø¹Ø©)
MAJOR_PATTERNS = [
    r"\bÙ†Ø¸Ù…\s+Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\s+Ø§Ù„Ø§Ø¯Ø§Ø±ÙŠ(?:Ù‡|Ø©)?\b",
    r"\bØ§Ø¯Ø§Ø±Ù‡?\s+Ù†Ø¸Ù…\s+Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\b",
    r"\bmis\b",
    r"\bmanagement\s+information\s+systems?\b"
]

# Ø§Ù„Ø¬Ù†Ø³ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
NAT_PATTERNS = [
    r"\bØ³Ø¹ÙˆØ¯ÙŠ(?:Ù‡)?\b",
    r"\bsaudi(?:\s+(?:arabia|national))?\b"
]

def find_matches(patterns, text):
    """ÙŠØ±Ø¬Ø¹: Ù…ÙˆØ¬ÙˆØ¯/ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ + Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø§Ù„ØªÙŠ ÙˆÙØ¬Ø¯Øª ÙØ¹Ù„Ø§Ù‹."""
    hits = []
    for p in patterns:
        for m in re.findall(p, text, flags=re.IGNORECASE):
            if isinstance(m, tuple):
                # Ù„Ùˆ Ø§Ù„Ø±Ø¬ÙƒØ³ ÙÙŠÙ‡ Ù…Ø¬Ù…ÙˆØ¹Ø§ØªØŒ Ù†Ø­ÙˆÙ„Ù‡Ø§ Ù„Ù†Øµ ÙˆØ§Ø­Ø¯
                m = " ".join([x for x in m if isinstance(x, str)])
            hits.append(m if isinstance(m, str) else "")
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    hits = [h.strip() for h in hits if h and h.strip()]
    return (len(hits) > 0), hits

def evaluate_cv(raw_text: str):
    """ÙŠØ±Ø¬Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© + ØªÙØµÙŠÙ„ Ø§Ù„Ø´Ø±ÙˆØ· + Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©."""
    norm = normalize_ar(raw_text)
    uni_ok, uni_hits   = find_matches(UNI_PATTERNS,   norm)
    major_ok, major_hits = find_matches(MAJOR_PATTERNS, norm)
    nat_ok, nat_hits   = find_matches(NAT_PATTERNS,   norm)

    all_ok = uni_ok and major_ok and nat_ok
    verdict = "âœ… Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ø´Ø±ÙˆØ·" if all_ok else "âŒ ØºÙŠØ± Ù…Ø·Ø§Ø¨Ù‚"

    detail = [
        ("Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", uni_ok, uni_hits),
        ("Ø§Ù„ØªØ®ØµØµ", major_ok, major_hits),
        ("Ø§Ù„Ø¬Ù†Ø³ÙŠØ©", nat_ok, nat_hits),
    ]
    return verdict, detail, norm

def render_detail(detail):
    """ÙŠØ¹Ø±Ø¶ ØªÙØµÙŠÙ„ Ø§Ù„Ø´Ø±ÙˆØ· + Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­."""
    for label, ok, hits in detail:
        st.write(f"**{label}:** {'âœ… Ù…ÙˆØ¬ÙˆØ¯' if ok else 'âŒ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯'}")
        if hits:
            with st.expander(f"Ø§Ù„Ø£Ù„ÙØ§Ø¸ Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ ÙÙŠ {label}"):
                for h in sorted(set(hits)):
                    st.code(h, language="text")

# ========= Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© =========
tab_single, tab_multi, tab_samples = st.tabs(["ØªØ­Ù‚Ù‚ Ù…Ù† Ø³ÙŠØ±Ø© ÙˆØ§Ø­Ø¯Ø©", "ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª", "Ø£Ù…Ø«Ù„Ø© Ø¬Ø§Ù‡Ø²Ø©"])

# --- ØªØ¨ÙˆÙŠØ¨: Ø³ÙŠØ±Ø© ÙˆØ§Ø­Ø¯Ø© ---
with tab_single:
    st.subheader("Ø£Ø¯Ø®Ù„ÙŠ Ù†Øµ Ø§Ù„Ø³ÙŠØ±Ø© Ø£Ùˆ Ø§Ø±ÙØ¹ÙŠ PDF")
    col1, col2 = st.columns(2)
    with col1:
        cv_text = st.text_area("Ù†Øµ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ø¥Ø°Ø§ Ø³ØªØ±ÙØ¹ÙŠÙ† PDF)", height=220, placeholder="Ø§Ù„Ø§Ø³Ù… ... Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ... Ø§Ù„ØªØ®ØµØµ ... Ø§Ù„Ø¬Ù†Ø³ÙŠØ© ...")
    with col2:
        one_file = st.file_uploader("Ø£Ùˆ Ø§Ø±ÙØ¹ÙŠ Ù…Ù„Ù PDF ÙˆØ§Ø­Ø¯", type=["pdf"], accept_multiple_files=False)

    if st.button("ØªØ­Ù‚Ù‘Ù‚ Ø§Ù„Ø¢Ù†", type="primary"):
        if one_file and not cv_text.strip():
            try:
                raw = extract_pdf_text(one_file.read())
            except Exception as e:
                st.error(f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù€ PDF: {e}")
                raw = ""
        else:
            raw = cv_text

        if not raw.strip():
            st.warning("ÙØ¶Ù„Ø§Ù‹ Ø£Ø¯Ø®Ù„ÙŠ Ù†ØµÙ‹Ø§ Ø£Ùˆ Ø§Ø±ÙØ¹ÙŠ Ù…Ù„Ù PDF.")
        else:
            verdict, detail, norm = evaluate_cv(raw)
            st.markdown(f"### Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {verdict}")
            render_detail(detail)
            with st.expander("Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡"):
                st.text(raw)

# --- ØªØ¨ÙˆÙŠØ¨: Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª ---
with tab_multi:
    st.subheader("Ø§Ø±ÙØ¹ÙŠ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª PDF ÙˆØ³ÙŠØªÙ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒÙ„ Ù…Ù„Ù")
    files = st.file_uploader("Ø§Ø±ÙØ¹ÙŠ Ù…Ù„ÙØ§Øª PDF", type=["pdf"], accept_multiple_files=True)

    if st.button("ØªØ­Ù‚Ù‘Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª", type="secondary"):
        if not files:
            st.warning("ÙØ¶Ù„Ø§Ù‹ Ø§Ø±ÙØ¹ÙŠ Ù…Ù„ÙÙ‹Ø§ ÙˆØ§Ø­Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„.")
        else:
            for idx, f in enumerate(files, start=1):
                st.divider()
                st.write(f"**Ø§Ù„Ù…Ù„Ù {idx}:** `{f.name}`")
                try:
                    raw = extract_pdf_text(f.read())
                except Exception as e:
                    st.error(f"ØªØ¹Ø°Ù‘Ø± Ù‚Ø±Ø§Ø¡Ø© `{f.name}`: {e}")
                    continue

                verdict, detail, norm = evaluate_cv(raw)
                st.markdown(f"**Ø§Ù„Ù†ØªÙŠØ¬Ø©:** {verdict}")
                render_detail(detail)

# --- ØªØ¨ÙˆÙŠØ¨: Ø£Ù…Ø«Ù„Ø© Ø¬Ø§Ù‡Ø²Ø© ---
with tab_samples:
    st.subheader("ØªØ¬Ø±Ø¨Ø© Ø¨Ø¹ÙŠÙ†Ø§Øª Ù†ØµÙŠØ©")
    samples = [
        ("Ø¹ÙŠÙ†Ø© 1 (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† âœ…)", "Ø§Ù„Ø¬Ø§Ù…Ø¹Ù‡: Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯\nØ§Ù„ØªØ®ØµØµ: Ù†Ø¸Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§Ø¯Ø§Ø±ÙŠÙ‡\nØ§Ù„Ø¬Ù†Ø³ÙŠÙ‡: Ø³Ø¹ÙˆØ¯ÙŠÙ‡"),
        ("Ø¹ÙŠÙ†Ø© 2 (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† âœ…)", "University: King Saud University\nMajor: MIS\nNationality: Saudi"),
        ("Ø¹ÙŠÙ†Ø© 3 (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† âŒ)", "Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©: Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø®Ø§Ù„Ø¯\nØ§Ù„ØªØ®ØµØµ: Ù…Ø­Ø§Ø³Ø¨Ø©\nØ§Ù„Ø¬Ù†Ø³ÙŠØ©: ØºÙŠØ± Ø³Ø¹ÙˆØ¯ÙŠ"),
    ]
    for title, s in samples:
        st.write(f"**{title}**")
        st.code(s, language="text")
        v, d, _ = evaluate_cv(s)
        st.write(f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {v}")
        render_detail(d)
        st.divider()

st.caption("ğŸ” Ù…Ù„Ø§Ø­Ø¸Ø©: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù€ PDF Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† ØµÙˆØ±Ø© Ù…Ù…Ø³ÙˆØ­Ø© (Ø¨Ø¯ÙˆÙ† Ù†Øµ)ØŒ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ÙŠÙ† Ù„Ø§Ø­Ù‚Ù‹Ø§ Ù„Ø¥Ø¶Ø§ÙØ© OCR. Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… OCR Ø¹Ù…Ø¯Ù‹Ø§ Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ù†Ø´Ø±.")
